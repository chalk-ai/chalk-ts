// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.6.1
//   protoc               unknown
// source: chalk/aggregate/v1/service.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import {
  type CallOptions,
  ChannelCredentials,
  Client,
  type ClientOptions,
  type ClientUnaryCall,
  type handleUnaryCall,
  makeGenericClientConstructor,
  Metadata,
  type ServiceError,
  type UntypedServiceImplementation,
} from "@grpc/grpc-js";
import {
  AggregateBackfillCostEstimate,
  AggregateBackfillJob,
  AggregateBackfillUserParams,
  AggregateBackfillWithCostEstimate,
  CronAggregateBackfill,
} from "./backfill.pb";
import { AggregateTimeSeries } from "./timeseries.pb";

export const protobufPackage = "chalk.aggregate.v1";

export interface PlanAggregateBackfillRequest {
  params: AggregateBackfillUserParams | undefined;
}

export interface PlanAggregateBackfillResponse {
  estimate: AggregateBackfillCostEstimate | undefined;
  errors: string[];
  backfills: AggregateBackfillWithCostEstimate[];
  aggregateBackfillId: string;
}

export interface GetAggregatesRequest {
  forFeatures: string[];
}

export interface GetAggregatesResponse {
  series: AggregateTimeSeries[];
  errors: string[];
}

export interface GetAggregateBackfillJobsRequest {
  limit: number;
  cursor: string;
  planHash?: string | undefined;
}

export interface GetAggregateBackfillJobsResponse {
  jobs: AggregateBackfillJob[];
  cursor: string;
}

export interface GetAggregateBackfillJobRequest {
  aggregateBackfillId: string;
}

export interface GetAggregateBackfillJobResponse {
  job: AggregateBackfillJob | undefined;
}

export interface GetCronAggregateBackfillRequest {
  /** @deprecated */
  cronAggregateBackfillId: string;
  planHash: string;
}

export interface GetCronAggregateBackfillResponse {
  cronAggregateBackfill: CronAggregateBackfill | undefined;
}

export interface GetActiveCronAggregateBackfillsRequest {
}

export interface CronAggregateBackfillWithLatestRun {
  cronAggregateBackfill: CronAggregateBackfill | undefined;
  latestJob: AggregateBackfillJob | undefined;
}

export interface GetActiveCronAggregateBackfillsResponse {
  cronAggregateBackfills: CronAggregateBackfillWithLatestRun[];
}

function createBasePlanAggregateBackfillRequest(): PlanAggregateBackfillRequest {
  return { params: undefined };
}

export const PlanAggregateBackfillRequest: MessageFns<PlanAggregateBackfillRequest> = {
  encode(message: PlanAggregateBackfillRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.params !== undefined) {
      AggregateBackfillUserParams.encode(message.params, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PlanAggregateBackfillRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePlanAggregateBackfillRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.params = AggregateBackfillUserParams.decode(reader, reader.uint32());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PlanAggregateBackfillRequest {
    return { params: isSet(object.params) ? AggregateBackfillUserParams.fromJSON(object.params) : undefined };
  },

  toJSON(message: PlanAggregateBackfillRequest): unknown {
    const obj: any = {};
    if (message.params !== undefined) {
      obj.params = AggregateBackfillUserParams.toJSON(message.params);
    }
    return obj;
  },
};

function createBasePlanAggregateBackfillResponse(): PlanAggregateBackfillResponse {
  return { estimate: undefined, errors: [], backfills: [], aggregateBackfillId: "" };
}

export const PlanAggregateBackfillResponse: MessageFns<PlanAggregateBackfillResponse> = {
  encode(message: PlanAggregateBackfillResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.estimate !== undefined) {
      AggregateBackfillCostEstimate.encode(message.estimate, writer.uint32(18).fork()).join();
    }
    for (const v of message.errors) {
      writer.uint32(34).string(v!);
    }
    for (const v of message.backfills) {
      AggregateBackfillWithCostEstimate.encode(v!, writer.uint32(50).fork()).join();
    }
    if (message.aggregateBackfillId !== "") {
      writer.uint32(58).string(message.aggregateBackfillId);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PlanAggregateBackfillResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePlanAggregateBackfillResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.estimate = AggregateBackfillCostEstimate.decode(reader, reader.uint32());
          continue;
        }
        case 4: {
          if (tag !== 34) {
            break;
          }

          message.errors.push(reader.string());
          continue;
        }
        case 6: {
          if (tag !== 50) {
            break;
          }

          message.backfills.push(AggregateBackfillWithCostEstimate.decode(reader, reader.uint32()));
          continue;
        }
        case 7: {
          if (tag !== 58) {
            break;
          }

          message.aggregateBackfillId = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PlanAggregateBackfillResponse {
    return {
      estimate: isSet(object.estimate) ? AggregateBackfillCostEstimate.fromJSON(object.estimate) : undefined,
      errors: globalThis.Array.isArray(object?.errors) ? object.errors.map((e: any) => globalThis.String(e)) : [],
      backfills: globalThis.Array.isArray(object?.backfills)
        ? object.backfills.map((e: any) => AggregateBackfillWithCostEstimate.fromJSON(e))
        : [],
      aggregateBackfillId: isSet(object.aggregateBackfillId) ? globalThis.String(object.aggregateBackfillId) : "",
    };
  },

  toJSON(message: PlanAggregateBackfillResponse): unknown {
    const obj: any = {};
    if (message.estimate !== undefined) {
      obj.estimate = AggregateBackfillCostEstimate.toJSON(message.estimate);
    }
    if (message.errors?.length) {
      obj.errors = message.errors;
    }
    if (message.backfills?.length) {
      obj.backfills = message.backfills.map((e) => AggregateBackfillWithCostEstimate.toJSON(e));
    }
    if (message.aggregateBackfillId !== "") {
      obj.aggregateBackfillId = message.aggregateBackfillId;
    }
    return obj;
  },
};

function createBaseGetAggregatesRequest(): GetAggregatesRequest {
  return { forFeatures: [] };
}

export const GetAggregatesRequest: MessageFns<GetAggregatesRequest> = {
  encode(message: GetAggregatesRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.forFeatures) {
      writer.uint32(10).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GetAggregatesRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGetAggregatesRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.forFeatures.push(reader.string());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GetAggregatesRequest {
    return {
      forFeatures: globalThis.Array.isArray(object?.forFeatures)
        ? object.forFeatures.map((e: any) => globalThis.String(e))
        : [],
    };
  },

  toJSON(message: GetAggregatesRequest): unknown {
    const obj: any = {};
    if (message.forFeatures?.length) {
      obj.forFeatures = message.forFeatures;
    }
    return obj;
  },
};

function createBaseGetAggregatesResponse(): GetAggregatesResponse {
  return { series: [], errors: [] };
}

export const GetAggregatesResponse: MessageFns<GetAggregatesResponse> = {
  encode(message: GetAggregatesResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.series) {
      AggregateTimeSeries.encode(v!, writer.uint32(10).fork()).join();
    }
    for (const v of message.errors) {
      writer.uint32(18).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GetAggregatesResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGetAggregatesResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.series.push(AggregateTimeSeries.decode(reader, reader.uint32()));
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.errors.push(reader.string());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GetAggregatesResponse {
    return {
      series: globalThis.Array.isArray(object?.series)
        ? object.series.map((e: any) => AggregateTimeSeries.fromJSON(e))
        : [],
      errors: globalThis.Array.isArray(object?.errors) ? object.errors.map((e: any) => globalThis.String(e)) : [],
    };
  },

  toJSON(message: GetAggregatesResponse): unknown {
    const obj: any = {};
    if (message.series?.length) {
      obj.series = message.series.map((e) => AggregateTimeSeries.toJSON(e));
    }
    if (message.errors?.length) {
      obj.errors = message.errors;
    }
    return obj;
  },
};

function createBaseGetAggregateBackfillJobsRequest(): GetAggregateBackfillJobsRequest {
  return { limit: 0, cursor: "", planHash: undefined };
}

export const GetAggregateBackfillJobsRequest: MessageFns<GetAggregateBackfillJobsRequest> = {
  encode(message: GetAggregateBackfillJobsRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.limit !== 0) {
      writer.uint32(8).int32(message.limit);
    }
    if (message.cursor !== "") {
      writer.uint32(18).string(message.cursor);
    }
    if (message.planHash !== undefined) {
      writer.uint32(26).string(message.planHash);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GetAggregateBackfillJobsRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGetAggregateBackfillJobsRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 8) {
            break;
          }

          message.limit = reader.int32();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.cursor = reader.string();
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.planHash = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GetAggregateBackfillJobsRequest {
    return {
      limit: isSet(object.limit) ? globalThis.Number(object.limit) : 0,
      cursor: isSet(object.cursor) ? globalThis.String(object.cursor) : "",
      planHash: isSet(object.planHash) ? globalThis.String(object.planHash) : undefined,
    };
  },

  toJSON(message: GetAggregateBackfillJobsRequest): unknown {
    const obj: any = {};
    if (message.limit !== 0) {
      obj.limit = Math.round(message.limit);
    }
    if (message.cursor !== "") {
      obj.cursor = message.cursor;
    }
    if (message.planHash !== undefined) {
      obj.planHash = message.planHash;
    }
    return obj;
  },
};

function createBaseGetAggregateBackfillJobsResponse(): GetAggregateBackfillJobsResponse {
  return { jobs: [], cursor: "" };
}

export const GetAggregateBackfillJobsResponse: MessageFns<GetAggregateBackfillJobsResponse> = {
  encode(message: GetAggregateBackfillJobsResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.jobs) {
      AggregateBackfillJob.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.cursor !== "") {
      writer.uint32(18).string(message.cursor);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GetAggregateBackfillJobsResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGetAggregateBackfillJobsResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.jobs.push(AggregateBackfillJob.decode(reader, reader.uint32()));
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.cursor = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GetAggregateBackfillJobsResponse {
    return {
      jobs: globalThis.Array.isArray(object?.jobs) ? object.jobs.map((e: any) => AggregateBackfillJob.fromJSON(e)) : [],
      cursor: isSet(object.cursor) ? globalThis.String(object.cursor) : "",
    };
  },

  toJSON(message: GetAggregateBackfillJobsResponse): unknown {
    const obj: any = {};
    if (message.jobs?.length) {
      obj.jobs = message.jobs.map((e) => AggregateBackfillJob.toJSON(e));
    }
    if (message.cursor !== "") {
      obj.cursor = message.cursor;
    }
    return obj;
  },
};

function createBaseGetAggregateBackfillJobRequest(): GetAggregateBackfillJobRequest {
  return { aggregateBackfillId: "" };
}

export const GetAggregateBackfillJobRequest: MessageFns<GetAggregateBackfillJobRequest> = {
  encode(message: GetAggregateBackfillJobRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.aggregateBackfillId !== "") {
      writer.uint32(10).string(message.aggregateBackfillId);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GetAggregateBackfillJobRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGetAggregateBackfillJobRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.aggregateBackfillId = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GetAggregateBackfillJobRequest {
    return {
      aggregateBackfillId: isSet(object.aggregateBackfillId) ? globalThis.String(object.aggregateBackfillId) : "",
    };
  },

  toJSON(message: GetAggregateBackfillJobRequest): unknown {
    const obj: any = {};
    if (message.aggregateBackfillId !== "") {
      obj.aggregateBackfillId = message.aggregateBackfillId;
    }
    return obj;
  },
};

function createBaseGetAggregateBackfillJobResponse(): GetAggregateBackfillJobResponse {
  return { job: undefined };
}

export const GetAggregateBackfillJobResponse: MessageFns<GetAggregateBackfillJobResponse> = {
  encode(message: GetAggregateBackfillJobResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.job !== undefined) {
      AggregateBackfillJob.encode(message.job, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GetAggregateBackfillJobResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGetAggregateBackfillJobResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.job = AggregateBackfillJob.decode(reader, reader.uint32());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GetAggregateBackfillJobResponse {
    return { job: isSet(object.job) ? AggregateBackfillJob.fromJSON(object.job) : undefined };
  },

  toJSON(message: GetAggregateBackfillJobResponse): unknown {
    const obj: any = {};
    if (message.job !== undefined) {
      obj.job = AggregateBackfillJob.toJSON(message.job);
    }
    return obj;
  },
};

function createBaseGetCronAggregateBackfillRequest(): GetCronAggregateBackfillRequest {
  return { cronAggregateBackfillId: "", planHash: "" };
}

export const GetCronAggregateBackfillRequest: MessageFns<GetCronAggregateBackfillRequest> = {
  encode(message: GetCronAggregateBackfillRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.cronAggregateBackfillId !== "") {
      writer.uint32(10).string(message.cronAggregateBackfillId);
    }
    if (message.planHash !== "") {
      writer.uint32(18).string(message.planHash);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GetCronAggregateBackfillRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGetCronAggregateBackfillRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.cronAggregateBackfillId = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.planHash = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GetCronAggregateBackfillRequest {
    return {
      cronAggregateBackfillId: isSet(object.cronAggregateBackfillId)
        ? globalThis.String(object.cronAggregateBackfillId)
        : "",
      planHash: isSet(object.planHash) ? globalThis.String(object.planHash) : "",
    };
  },

  toJSON(message: GetCronAggregateBackfillRequest): unknown {
    const obj: any = {};
    if (message.cronAggregateBackfillId !== "") {
      obj.cronAggregateBackfillId = message.cronAggregateBackfillId;
    }
    if (message.planHash !== "") {
      obj.planHash = message.planHash;
    }
    return obj;
  },
};

function createBaseGetCronAggregateBackfillResponse(): GetCronAggregateBackfillResponse {
  return { cronAggregateBackfill: undefined };
}

export const GetCronAggregateBackfillResponse: MessageFns<GetCronAggregateBackfillResponse> = {
  encode(message: GetCronAggregateBackfillResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.cronAggregateBackfill !== undefined) {
      CronAggregateBackfill.encode(message.cronAggregateBackfill, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GetCronAggregateBackfillResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGetCronAggregateBackfillResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.cronAggregateBackfill = CronAggregateBackfill.decode(reader, reader.uint32());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GetCronAggregateBackfillResponse {
    return {
      cronAggregateBackfill: isSet(object.cronAggregateBackfill)
        ? CronAggregateBackfill.fromJSON(object.cronAggregateBackfill)
        : undefined,
    };
  },

  toJSON(message: GetCronAggregateBackfillResponse): unknown {
    const obj: any = {};
    if (message.cronAggregateBackfill !== undefined) {
      obj.cronAggregateBackfill = CronAggregateBackfill.toJSON(message.cronAggregateBackfill);
    }
    return obj;
  },
};

function createBaseGetActiveCronAggregateBackfillsRequest(): GetActiveCronAggregateBackfillsRequest {
  return {};
}

export const GetActiveCronAggregateBackfillsRequest: MessageFns<GetActiveCronAggregateBackfillsRequest> = {
  encode(_: GetActiveCronAggregateBackfillsRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GetActiveCronAggregateBackfillsRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGetActiveCronAggregateBackfillsRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(_: any): GetActiveCronAggregateBackfillsRequest {
    return {};
  },

  toJSON(_: GetActiveCronAggregateBackfillsRequest): unknown {
    const obj: any = {};
    return obj;
  },
};

function createBaseCronAggregateBackfillWithLatestRun(): CronAggregateBackfillWithLatestRun {
  return { cronAggregateBackfill: undefined, latestJob: undefined };
}

export const CronAggregateBackfillWithLatestRun: MessageFns<CronAggregateBackfillWithLatestRun> = {
  encode(message: CronAggregateBackfillWithLatestRun, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.cronAggregateBackfill !== undefined) {
      CronAggregateBackfill.encode(message.cronAggregateBackfill, writer.uint32(10).fork()).join();
    }
    if (message.latestJob !== undefined) {
      AggregateBackfillJob.encode(message.latestJob, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CronAggregateBackfillWithLatestRun {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCronAggregateBackfillWithLatestRun();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.cronAggregateBackfill = CronAggregateBackfill.decode(reader, reader.uint32());
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.latestJob = AggregateBackfillJob.decode(reader, reader.uint32());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CronAggregateBackfillWithLatestRun {
    return {
      cronAggregateBackfill: isSet(object.cronAggregateBackfill)
        ? CronAggregateBackfill.fromJSON(object.cronAggregateBackfill)
        : undefined,
      latestJob: isSet(object.latestJob) ? AggregateBackfillJob.fromJSON(object.latestJob) : undefined,
    };
  },

  toJSON(message: CronAggregateBackfillWithLatestRun): unknown {
    const obj: any = {};
    if (message.cronAggregateBackfill !== undefined) {
      obj.cronAggregateBackfill = CronAggregateBackfill.toJSON(message.cronAggregateBackfill);
    }
    if (message.latestJob !== undefined) {
      obj.latestJob = AggregateBackfillJob.toJSON(message.latestJob);
    }
    return obj;
  },
};

function createBaseGetActiveCronAggregateBackfillsResponse(): GetActiveCronAggregateBackfillsResponse {
  return { cronAggregateBackfills: [] };
}

export const GetActiveCronAggregateBackfillsResponse: MessageFns<GetActiveCronAggregateBackfillsResponse> = {
  encode(message: GetActiveCronAggregateBackfillsResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.cronAggregateBackfills) {
      CronAggregateBackfillWithLatestRun.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GetActiveCronAggregateBackfillsResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGetActiveCronAggregateBackfillsResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.cronAggregateBackfills.push(CronAggregateBackfillWithLatestRun.decode(reader, reader.uint32()));
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GetActiveCronAggregateBackfillsResponse {
    return {
      cronAggregateBackfills: globalThis.Array.isArray(object?.cronAggregateBackfills)
        ? object.cronAggregateBackfills.map((e: any) => CronAggregateBackfillWithLatestRun.fromJSON(e))
        : [],
    };
  },

  toJSON(message: GetActiveCronAggregateBackfillsResponse): unknown {
    const obj: any = {};
    if (message.cronAggregateBackfills?.length) {
      obj.cronAggregateBackfills = message.cronAggregateBackfills.map((e) =>
        CronAggregateBackfillWithLatestRun.toJSON(e)
      );
    }
    return obj;
  },
};

export type AggregateServiceService = typeof AggregateServiceService;
export const AggregateServiceService = {
  /**
   * PlanAggregateBackfill determines the estimated resources needed to backfill
   * an aggregate.
   *
   * This method is a duplicate of the PlanAggregateBackfill method
   * in the query_server.proto file. We should remove the query_server.proto method
   * and move that request to this service instead.
   * buf:lint:ignore RPC_REQUEST_RESPONSE_UNIQUE
   */
  planAggregateBackfill: {
    path: "/chalk.aggregate.v1.AggregateService/PlanAggregateBackfill",
    requestStream: false,
    responseStream: false,
    requestSerialize: (value: PlanAggregateBackfillRequest) =>
      Buffer.from(PlanAggregateBackfillRequest.encode(value).finish()),
    requestDeserialize: (value: Buffer) => PlanAggregateBackfillRequest.decode(value),
    responseSerialize: (value: PlanAggregateBackfillResponse) =>
      Buffer.from(PlanAggregateBackfillResponse.encode(value).finish()),
    responseDeserialize: (value: Buffer) => PlanAggregateBackfillResponse.decode(value),
  },
  /**
   * This method is a duplicate of the PlanAggregateBackfill method
   * in the query_server.proto file. We should remove the query_server.proto method
   * and move that request to this service instead.
   * buf:lint:ignore RPC_REQUEST_RESPONSE_UNIQUE
   */
  getAggregates: {
    path: "/chalk.aggregate.v1.AggregateService/GetAggregates",
    requestStream: false,
    responseStream: false,
    requestSerialize: (value: GetAggregatesRequest) => Buffer.from(GetAggregatesRequest.encode(value).finish()),
    requestDeserialize: (value: Buffer) => GetAggregatesRequest.decode(value),
    responseSerialize: (value: GetAggregatesResponse) => Buffer.from(GetAggregatesResponse.encode(value).finish()),
    responseDeserialize: (value: Buffer) => GetAggregatesResponse.decode(value),
  },
  getAggregateBackfillJobs: {
    path: "/chalk.aggregate.v1.AggregateService/GetAggregateBackfillJobs",
    requestStream: false,
    responseStream: false,
    requestSerialize: (value: GetAggregateBackfillJobsRequest) =>
      Buffer.from(GetAggregateBackfillJobsRequest.encode(value).finish()),
    requestDeserialize: (value: Buffer) => GetAggregateBackfillJobsRequest.decode(value),
    responseSerialize: (value: GetAggregateBackfillJobsResponse) =>
      Buffer.from(GetAggregateBackfillJobsResponse.encode(value).finish()),
    responseDeserialize: (value: Buffer) => GetAggregateBackfillJobsResponse.decode(value),
  },
  getAggregateBackfillJob: {
    path: "/chalk.aggregate.v1.AggregateService/GetAggregateBackfillJob",
    requestStream: false,
    responseStream: false,
    requestSerialize: (value: GetAggregateBackfillJobRequest) =>
      Buffer.from(GetAggregateBackfillJobRequest.encode(value).finish()),
    requestDeserialize: (value: Buffer) => GetAggregateBackfillJobRequest.decode(value),
    responseSerialize: (value: GetAggregateBackfillJobResponse) =>
      Buffer.from(GetAggregateBackfillJobResponse.encode(value).finish()),
    responseDeserialize: (value: Buffer) => GetAggregateBackfillJobResponse.decode(value),
  },
  getCronAggregateBackfill: {
    path: "/chalk.aggregate.v1.AggregateService/GetCronAggregateBackfill",
    requestStream: false,
    responseStream: false,
    requestSerialize: (value: GetCronAggregateBackfillRequest) =>
      Buffer.from(GetCronAggregateBackfillRequest.encode(value).finish()),
    requestDeserialize: (value: Buffer) => GetCronAggregateBackfillRequest.decode(value),
    responseSerialize: (value: GetCronAggregateBackfillResponse) =>
      Buffer.from(GetCronAggregateBackfillResponse.encode(value).finish()),
    responseDeserialize: (value: Buffer) => GetCronAggregateBackfillResponse.decode(value),
  },
  getActiveCronAggregateBackfills: {
    path: "/chalk.aggregate.v1.AggregateService/GetActiveCronAggregateBackfills",
    requestStream: false,
    responseStream: false,
    requestSerialize: (value: GetActiveCronAggregateBackfillsRequest) =>
      Buffer.from(GetActiveCronAggregateBackfillsRequest.encode(value).finish()),
    requestDeserialize: (value: Buffer) => GetActiveCronAggregateBackfillsRequest.decode(value),
    responseSerialize: (value: GetActiveCronAggregateBackfillsResponse) =>
      Buffer.from(GetActiveCronAggregateBackfillsResponse.encode(value).finish()),
    responseDeserialize: (value: Buffer) => GetActiveCronAggregateBackfillsResponse.decode(value),
  },
} as const;

export interface AggregateServiceServer extends UntypedServiceImplementation {
  /**
   * PlanAggregateBackfill determines the estimated resources needed to backfill
   * an aggregate.
   *
   * This method is a duplicate of the PlanAggregateBackfill method
   * in the query_server.proto file. We should remove the query_server.proto method
   * and move that request to this service instead.
   * buf:lint:ignore RPC_REQUEST_RESPONSE_UNIQUE
   */
  planAggregateBackfill: handleUnaryCall<PlanAggregateBackfillRequest, PlanAggregateBackfillResponse>;
  /**
   * This method is a duplicate of the PlanAggregateBackfill method
   * in the query_server.proto file. We should remove the query_server.proto method
   * and move that request to this service instead.
   * buf:lint:ignore RPC_REQUEST_RESPONSE_UNIQUE
   */
  getAggregates: handleUnaryCall<GetAggregatesRequest, GetAggregatesResponse>;
  getAggregateBackfillJobs: handleUnaryCall<GetAggregateBackfillJobsRequest, GetAggregateBackfillJobsResponse>;
  getAggregateBackfillJob: handleUnaryCall<GetAggregateBackfillJobRequest, GetAggregateBackfillJobResponse>;
  getCronAggregateBackfill: handleUnaryCall<GetCronAggregateBackfillRequest, GetCronAggregateBackfillResponse>;
  getActiveCronAggregateBackfills: handleUnaryCall<
    GetActiveCronAggregateBackfillsRequest,
    GetActiveCronAggregateBackfillsResponse
  >;
}

export interface AggregateServiceClient extends Client {
  /**
   * PlanAggregateBackfill determines the estimated resources needed to backfill
   * an aggregate.
   *
   * This method is a duplicate of the PlanAggregateBackfill method
   * in the query_server.proto file. We should remove the query_server.proto method
   * and move that request to this service instead.
   * buf:lint:ignore RPC_REQUEST_RESPONSE_UNIQUE
   */
  planAggregateBackfill(
    request: PlanAggregateBackfillRequest,
    callback: (error: ServiceError | null, response: PlanAggregateBackfillResponse) => void,
  ): ClientUnaryCall;
  planAggregateBackfill(
    request: PlanAggregateBackfillRequest,
    metadata: Metadata,
    callback: (error: ServiceError | null, response: PlanAggregateBackfillResponse) => void,
  ): ClientUnaryCall;
  planAggregateBackfill(
    request: PlanAggregateBackfillRequest,
    metadata: Metadata,
    options: Partial<CallOptions>,
    callback: (error: ServiceError | null, response: PlanAggregateBackfillResponse) => void,
  ): ClientUnaryCall;
  /**
   * This method is a duplicate of the PlanAggregateBackfill method
   * in the query_server.proto file. We should remove the query_server.proto method
   * and move that request to this service instead.
   * buf:lint:ignore RPC_REQUEST_RESPONSE_UNIQUE
   */
  getAggregates(
    request: GetAggregatesRequest,
    callback: (error: ServiceError | null, response: GetAggregatesResponse) => void,
  ): ClientUnaryCall;
  getAggregates(
    request: GetAggregatesRequest,
    metadata: Metadata,
    callback: (error: ServiceError | null, response: GetAggregatesResponse) => void,
  ): ClientUnaryCall;
  getAggregates(
    request: GetAggregatesRequest,
    metadata: Metadata,
    options: Partial<CallOptions>,
    callback: (error: ServiceError | null, response: GetAggregatesResponse) => void,
  ): ClientUnaryCall;
  getAggregateBackfillJobs(
    request: GetAggregateBackfillJobsRequest,
    callback: (error: ServiceError | null, response: GetAggregateBackfillJobsResponse) => void,
  ): ClientUnaryCall;
  getAggregateBackfillJobs(
    request: GetAggregateBackfillJobsRequest,
    metadata: Metadata,
    callback: (error: ServiceError | null, response: GetAggregateBackfillJobsResponse) => void,
  ): ClientUnaryCall;
  getAggregateBackfillJobs(
    request: GetAggregateBackfillJobsRequest,
    metadata: Metadata,
    options: Partial<CallOptions>,
    callback: (error: ServiceError | null, response: GetAggregateBackfillJobsResponse) => void,
  ): ClientUnaryCall;
  getAggregateBackfillJob(
    request: GetAggregateBackfillJobRequest,
    callback: (error: ServiceError | null, response: GetAggregateBackfillJobResponse) => void,
  ): ClientUnaryCall;
  getAggregateBackfillJob(
    request: GetAggregateBackfillJobRequest,
    metadata: Metadata,
    callback: (error: ServiceError | null, response: GetAggregateBackfillJobResponse) => void,
  ): ClientUnaryCall;
  getAggregateBackfillJob(
    request: GetAggregateBackfillJobRequest,
    metadata: Metadata,
    options: Partial<CallOptions>,
    callback: (error: ServiceError | null, response: GetAggregateBackfillJobResponse) => void,
  ): ClientUnaryCall;
  getCronAggregateBackfill(
    request: GetCronAggregateBackfillRequest,
    callback: (error: ServiceError | null, response: GetCronAggregateBackfillResponse) => void,
  ): ClientUnaryCall;
  getCronAggregateBackfill(
    request: GetCronAggregateBackfillRequest,
    metadata: Metadata,
    callback: (error: ServiceError | null, response: GetCronAggregateBackfillResponse) => void,
  ): ClientUnaryCall;
  getCronAggregateBackfill(
    request: GetCronAggregateBackfillRequest,
    metadata: Metadata,
    options: Partial<CallOptions>,
    callback: (error: ServiceError | null, response: GetCronAggregateBackfillResponse) => void,
  ): ClientUnaryCall;
  getActiveCronAggregateBackfills(
    request: GetActiveCronAggregateBackfillsRequest,
    callback: (error: ServiceError | null, response: GetActiveCronAggregateBackfillsResponse) => void,
  ): ClientUnaryCall;
  getActiveCronAggregateBackfills(
    request: GetActiveCronAggregateBackfillsRequest,
    metadata: Metadata,
    callback: (error: ServiceError | null, response: GetActiveCronAggregateBackfillsResponse) => void,
  ): ClientUnaryCall;
  getActiveCronAggregateBackfills(
    request: GetActiveCronAggregateBackfillsRequest,
    metadata: Metadata,
    options: Partial<CallOptions>,
    callback: (error: ServiceError | null, response: GetActiveCronAggregateBackfillsResponse) => void,
  ): ClientUnaryCall;
}

export const AggregateServiceClient = makeGenericClientConstructor(
  AggregateServiceService,
  "chalk.aggregate.v1.AggregateService",
) as unknown as {
  new (address: string, credentials: ChannelCredentials, options?: Partial<ClientOptions>): AggregateServiceClient;
  service: typeof AggregateServiceService;
  serviceName: string;
};

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
}
